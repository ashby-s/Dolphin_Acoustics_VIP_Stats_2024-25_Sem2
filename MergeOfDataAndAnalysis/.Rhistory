N <- 42
n <- 6
simprs <- sample(x=1,size=n,replace=FALSE)
simprs <- sample(x=1:N,size=n,replace=FALSE)
K <- round(N/n)
q <- sample(x=1:k,size=1)
q <- sample(x=1:K,size=1)
systrs <- seq(from=q, to=N, by=K)
dbinom(x=2, size=10, prob=0.1)
dbinom(x=4, size=10, prob=0.25)
dbinom(x=1, size=4, prob=0.25)
dbinom(x=2, size=4, prob=0.25)
dbinom(x=3, size=4, prob=0.25)
dbinom(x=4, size=4, prob=0.25)
dbinom(x=5, size=4, prob=0.25)
dbinom(x=4, size=4, prob=0.25)
dbinom(x=0, size=4, prob=0.25)
sum(dbinom(x=(0:4), size=4, prob=0.25))
0:4
sum(dbinom(x=(0:4), size=4, prob=0.25))
(dbinom(x=(0:4), size=4, prob=0.25))
(dbinom(x=(0:2), size=4, prob=0.25))
sum((dbinom(x=(0:2), size=4, prob=0.25)))
(pbinom(x=2, size=4, prob=0.25))
(pbinom(q=2, size=4, prob=0.25))
pbinom(q=2, size=4, prob=0.25)
dpois(0,1.7)
1-ppois(1,1.7)
ppois(2,1.7,lower.tail = FALSE)
ppois(1,1.7,lower.tail = FALSE)
factorial(x=4)
n <- 4
n^2
r <- 2
factorial(n)/factorial(n-r)
choose(n=4,k=2)
choose(n=19,k=6)
choose(n=49,k=6)
pnorm(q=8, mean=6.3,sd=1.72)
pnorm(q=4, mean=6.3,sd=1.72)
pnorm(q=8, mean=6.3,sd=1.72) - pnorm(q=4, mean=6.3, sd=1.72)
dnorm(q=4, mean=6.3,sd=1.72)
dnorm(x=6.3, mean=6.3,sd=1.72)
?dnorm
rnorm(n=1, mean=6.3, sd=1.72)
pnorm(q=9,6.3,sqrt(2.958))-pnorm(q=3,6.3,sqrt(2.958))
pnorm(q=9,6.3,sqrt(2.958)) - pnorm(q=3,6.3,sqrt(2.958))
pnorm(q=9,6.3,sqrt(2.958)) - pnorm(q=3,6.3,sqrt(2.958))
0.9417654 - 0.02750898
#Possible Outcomes
num <- 1:6
num
#One throw of a fair die
sample(1,num,replace=TRUE)
#One throw of a fair die
sample(x=num,replace=TRUE)
#One throw of a fair die
sample(x=num,size=1)
#Set maximum number of throws
trials <- 100
throws <- sample(x=num,size=trials,replace=TRUE)
throws
head(throws)
table(throws)
head(throws)
table(throws)
barplot(table(throws))
#To find the Mean:
mean(throws)
# calculate the proportions of each outcome to obtain an empirical value for Pr(X = x)
emp.pmf <- table(throws)/trials
emp.pmf
#The expected number is obtained from:
exp.num <- sum(num*emp.pmf)
exp.num
#to calculate the ‘theoretical’ expected value:
prob <- 1/6
sum(num*prob)
throw6 <- ifelse(throws==6, yes = 1, no=0)
head(throw6)
head(throw)
head(throw6)
head(throws)
#to add up the number of sixes sequentially
cum.throw6 <- cumsum(throw6)
head(cum.throw6)
cum.throw6
head(cum.throw6)
# If we divide this score by the number of trials up to that point we can see how the proportion of sixes
#changes as the number of trials increases
ntrials <- 1:maxn
# If we divide this score by the number of trials up to that point we can see how the proportion of sixes
#changes as the number of trials increases
ntrials <- 1:trials
head(ntrials)
head(prop.throw6)
# If we divide this score by the number of trials up to that point we can see how the proportion of sixes
#changes as the number of trials increases
ntrials <- 1:trials
head(ntrials)
prop.throw6 <- cum.throw6/ntrials
head(prop.throw6)
#To plot the relative frequencies
plot(x=ntrials,y=prop.throw6,type = "b",xlab = "Number of throws",ylab="Proportion of sixes")
#Add mathematical probability
abline(h=1/6,lty=2)
#To plot the relative frequencies
plot(x=ntrials,y=prop.throw6,type = "b",xlab = "Number of throws",ylab="Proportion of sixes")
#Add mathematical probability
abline(h=1/6,lty=2)
#Check what the last proportion was:
tail(prop.throw6)
#Probability of 1 successes in 10 trials:
choose(n=10,k=1)
#Probability of 2 successes in 4 trials:
choose(n=4,k=2)
#Probability of 1 successes in 10 trials:
choose(n=10,k=1)
#Combination of 3 successes in 10 trials:
choose(n=10,k=3)
#Probability of female lamb
p.success <- 0.5
#Number of trials
n.ewe <- 3
#Create a data frame of all possible outcomes
lambs <- data.frame(x=0:n.ewe)
#Print object
lams
#Print object
lambs
(X=x)
#Pr(X=x)
lambs$prob.x <- dbinom(x=lambs$x,size=n.ewe,p=p.success)
lambs
#Add probabilities
plot(lambs$x,lambs$prob.x,type = "h",lwd=2,xlab="X",ylab"Probability of X")
#Add probabilities
plot(lambs$x,lambs$prob.x,type = "h",lwd=2,xlab="X",ylab="Probability of X")
#Add CDF
lambs$cdf.x <- pbinom(q=lambs$x,size=n.ewe,p=p.success)
lambs
#Add probabilities
plot(lambs$x,lambs$cdf.x,type = "h",lwd=2,xlab = "X",ylab = "Cumulative probability")
#Expectation
expected.x <- n.ewe * p.success
expected.x
#Also given by sum of (xPr(X =x))
sum(lambs$x * lambs$prob.x)
#We can also use similar calc to find variance
#Variance
var.x <- n.ewe * p.success * (1-p.success)
var x
#We can also use similar calc to find variance
#Variance
var.x <- n.ewe * p.success * (1-p.success)
var.x
#Also given my sum of ((x-E(x))^2*Pr(X=x))
var1 <- (lambs$x - expected.x)^2
sum(var1*lambs$prob.x)
#SD is just square root of variance
sqrt(var.x)
##Changing Shape of the Binomial
p.success <- 0.2
n <- 5
pmf <- dbinom(x=0:n, size=n, prob=p.success)
plot(0:n,pmf,type = "h",xlab="x")
#Probability of female lamb
p.success <- 0.5
#Number of trials
n.ewe <- 3
#Create a data frame of all possible outcomes
lambs <- data.frame(x=0:n.ewe)
#Print object
lambs
#Pr(X=x)
lambs$prob.x <- dbinom(x=lambs$x,size=n.ewe,p=p.success)
lambs
#Add probabilities
plot(lambs$x,lambs$prob.x,type = "h",lwd=2,xlab="X",ylab="Probability of X")
##Changing Shape of the Binomial
p.success <- 0.2
n <- 5
pmf <- dbinom(x=0:n, size=n, prob=p.success)
plot(0:n,pmf,type = "h",xlab="x")
#Specify mean rate of eruptions per year
mean.ve <- 50
#Create a data frame of number of eruptions (0 to 100)
volcano <- data.frame(num=0:100)
#Calculate probabilities
volcano$prob <- dpois(x=volcano$num,lambda=mean.ve)
#Plot probabilities
plot(volcano$num,volcano$prob, type="h",xlab="Number of eruptions",ylab="probability")
#Specify mean rate of eruptions per year
mean.ve <- 2
#Create a data frame of number of eruptions (0 to 100)
volcano <- data.frame(num=0:100)
#Calculate probabilities
volcano$prob <- dpois(x=volcano$num,lambda=mean.ve)
#Plot probabilities
plot(volcano$num,volcano$prob, type="h",xlab="Number of eruptions",ylab="probability")
#Specify mean rate of eruptions per year
mean.ve <- 50
#Create a data frame of number of eruptions (0 to 100)
volcano <- data.frame(num=0:100)
#Calculate probabilities
volcano$prob <- dpois(x=volcano$num,lambda=mean.ve)
#Plot probabilities
plot(volcano$num,volcano$prob, type="h",xlab="Number of eruptions",ylab="probability")
#Number of Students
maxn <- 5
days <- seq(from=365, to=365-maxn+1, by=-1)
days
#Probability nobody shares the same birthday:
prod(days)/(365^maxn)
#Probability that at least 2 people share the same birthday
1-prod(days)/(365^maxn)
#Probability at least 2 people out of 30 share the same birthday
maxn <- 30
days <- seq(from=365, to=365-maxn+1, by=-1)
1-prod(days)/(365^maxn)
prob <- 1/8
sum(num*prob)
num <- 1:8
sum(num*prob)
choose(10,2)
choose(10,2)
sqrt(50*0.3*0.7)
pbinom(15,50,0.3)
1-pbinom(15,50,0.3)
1-pbinom(14,50,0.3)
ppois(5,2)-ppois(3,2)
nval <- 100
n.success <- rpois(n=nval, lambda=2)
table(n.success)
n.success
#Probability at least 2 people out of 50 share the same birthday
maxn <- 30
days <- seq(from=365, to=365-maxn+1, by=-1)
1-prod(days)/(365^maxn)
#Probability at least 2 people out of 50 share the same birthday
maxn <- 50
days <- seq(from=365, to=365-maxn+1, by=-1)
1-prod(days)/(365^maxn)
?bbinom
?rbinom
1-pbinom(0,5,1/200)
dpois(0,3)
dpois(0,6)
?TukeyHSD
import pandas as pd
import matplotlib.pyplot as plt
# Load necessary libraries
library(readxl)
library(ggplot2)
# Load the Excel file
df <- read_excel("your_file.xlsx")
# Load necessary libraries
library(readxl)
library(ggplot2)
# Load the Excel file
df <- read_excel("GBR sea level reconstruction data for students.xlsx")
ls
cd
source("C:/Users/twink_cxd1o43/OneDrive/Desktop/University Notes + Work/Year 2/Dolphin Acoustics/R_Files_as696/Dolphin_Acoustics_VIP_Stats_2024-25_Sem2/MergeOfDataAndAnalysis/ARTwarp_ROCCA_merge.R", echo=TRUE)
setwd("C:/Users/twink_cxd1o43/OneDrive/Desktop/University Notes + Work/Year 2/Dolphin Acoustics/R_Files_as696/Dolphin_Acoustics_VIP_Stats_2024-25_Sem2/MergeOfDataAndAnalysis")
source("C:/Users/twink_cxd1o43/OneDrive/Desktop/University Notes + Work/Year 2/Dolphin Acoustics/R_Files_as696/Dolphin_Acoustics_VIP_Stats_2024-25_Sem2/MergeOfDataAndAnalysis/ARTwarp_ROCCA_merge.R", echo=TRUE)
cd
source("C:/Users/twink_cxd1o43/OneDrive/Desktop/University Notes + Work/Year 2/Dolphin Acoustics/R_Files_as696/Dolphin_Acoustics_VIP_Stats_2024-25_Sem2/MergeOfDataAndAnalysis/ARTwarp_ROCCA_merge.R", echo=TRUE)
source("C:/Users/twink_cxd1o43/OneDrive/Desktop/University Notes + Work/Year 2/Dolphin Acoustics/R_Files_as696/Dolphin_Acoustics_VIP_Stats_2024-25_Sem2/MergeOfDataAndAnalysis/Random_Forest_Analysis.R", echo=TRUE)
train_data <- df_selected[train_index, ]
# Split data into training (80%) and testing (20%) sets
set.seed(123)  # For reproducibility
source("C:/Users/twink_cxd1o43/OneDrive/Desktop/University Notes + Work/Year 2/Dolphin Acoustics/R_Files_as696/Dolphin_Acoustics_VIP_Stats_2024-25_Sem2/MergeOfDataAndAnalysis/RF_Accuracy_Test.R", echo=TRUE)
source("C:/Users/twink_cxd1o43/OneDrive/Desktop/University Notes + Work/Year 2/Dolphin Acoustics/R_Files_as696/Dolphin_Acoustics_VIP_Stats_2024-25_Sem2/MergeOfDataAndAnalysis/RF_Accuracy_Test.R", echo=TRUE)
install.packages("caret")
source("C:/Users/twink_cxd1o43/OneDrive/Desktop/University Notes + Work/Year 2/Dolphin Acoustics/R_Files_as696/Dolphin_Acoustics_VIP_Stats_2024-25_Sem2/MergeOfDataAndAnalysis/RF_Accuracy_Test.R", echo=TRUE)
# Load caret package if not already installed
# install.packages("caret")
library(caret)
# Load caret package if not already installed
install.packages("caret")
install.packages("caret")
install.packages("caret")
install.packages("caret")
source("C:/Users/twink_cxd1o43/OneDrive/Desktop/University Notes + Work/Year 2/Dolphin Acoustics/R_Files_as696/Dolphin_Acoustics_VIP_Stats_2024-25_Sem2/MergeOfDataAndAnalysis/RF_Accuracy_Test.R", echo=TRUE)
source("C:/Users/twink_cxd1o43/OneDrive/Desktop/University Notes + Work/Year 2/Dolphin Acoustics/R_Files_as696/Dolphin_Acoustics_VIP_Stats_2024-25_Sem2/MergeOfDataAndAnalysis/RF_Accuracy_Test.R", echo=TRUE)
# Ensure 'category' is a factor
df_selected$category <- as.factor(df_selected$category)
# Split data into training (80%) and testing (20%) sets
set.seed(456)  # For reproducibility
train_index <- sample(1:nrow(df_selected), 0.8 * nrow(df_selected))  # 80% for training
train_data <- df_selected[train_index, ]
test_data <- df_selected[-train_index, ]
# Drop empty factor levels in 'category'
train_data$category <- droplevels(train_data$category)
# Train the Random Forest model
library(randomForest)
model <- randomForest(category ~ ., data = train_data, ntree = 500, importance = TRUE)
# Predict on test data
predictions <- predict(model, newdata = test_data)
# Load caret package if not already installed
# install.packages("caret")
library(caret)
# Confusion matrix
conf_matrix <- confusionMatrix(predictions, test_data$category)
print(conf_matrix)
# Extract accuracy
accuracy <- conf_matrix$overall['Accuracy']
print(paste("Model Accuracy:", round(accuracy * 100, 2), "%"))
source("C:/Users/twink_cxd1o43/OneDrive/Desktop/University Notes + Work/Year 2/Dolphin Acoustics/R_Files_as696/Dolphin_Acoustics_VIP_Stats_2024-25_Sem2/MergeOfDataAndAnalysis/RF_Accuracy_Test.R", echo=TRUE)
source("C:/Users/twink_cxd1o43/OneDrive/Desktop/University Notes + Work/Year 2/Dolphin Acoustics/R_Files_as696/Dolphin_Acoustics_VIP_Stats_2024-25_Sem2/MergeOfDataAndAnalysis/RF_Accuracy_Test.R", echo=TRUE)
# Load caret package if not already installed
# install.packages("caret")
library(caret)
source("C:/Users/twink_cxd1o43/OneDrive/Desktop/University Notes + Work/Year 2/Dolphin Acoustics/R_Files_as696/Dolphin_Acoustics_VIP_Stats_2024-25_Sem2/MergeOfDataAndAnalysis/RF_Accuracy_Test.R", echo=TRUE)
train_data$category <- factor(train_data$category, levels = levels(df_selected$category))
test_data$category <- factor(test_data$category, levels = levels(df_selected$category))
source("C:/Users/twink_cxd1o43/OneDrive/Desktop/University Notes + Work/Year 2/Dolphin Acoustics/R_Files_as696/Dolphin_Acoustics_VIP_Stats_2024-25_Sem2/MergeOfDataAndAnalysis/RF_Accuracy_Test.R", echo=TRUE)
source("C:/Users/twink_cxd1o43/OneDrive/Desktop/University Notes + Work/Year 2/Dolphin Acoustics/R_Files_as696/Dolphin_Acoustics_VIP_Stats_2024-25_Sem2/MergeOfDataAndAnalysis/RF_Accuracy_Test.R", echo=TRUE)
source("C:/Users/twink_cxd1o43/OneDrive/Desktop/University Notes + Work/Year 2/Dolphin Acoustics/R_Files_as696/Dolphin_Acoustics_VIP_Stats_2024-25_Sem2/MergeOfDataAndAnalysis/RF_Accuracy_Test.R", echo=TRUE)
source("C:/Users/twink_cxd1o43/OneDrive/Desktop/University Notes + Work/Year 2/Dolphin Acoustics/R_Files_as696/Dolphin_Acoustics_VIP_Stats_2024-25_Sem2/MergeOfDataAndAnalysis/RF_Accuracy_Test.R", echo=TRUE)
source("C:/Users/twink_cxd1o43/OneDrive/Desktop/University Notes + Work/Year 2/Dolphin Acoustics/R_Files_as696/Dolphin_Acoustics_VIP_Stats_2024-25_Sem2/MergeOfDataAndAnalysis/RF_Accuracy_Test.R", echo=TRUE)
source("C:/Users/twink_cxd1o43/OneDrive/Desktop/University Notes + Work/Year 2/Dolphin Acoustics/R_Files_as696/Dolphin_Acoustics_VIP_Stats_2024-25_Sem2/MergeOfDataAndAnalysis/RF_Accuracy_Test.R", echo=TRUE)
source("C:/Users/twink_cxd1o43/OneDrive/Desktop/University Notes + Work/Year 2/Dolphin Acoustics/R_Files_as696/Dolphin_Acoustics_VIP_Stats_2024-25_Sem2/MergeOfDataAndAnalysis/RF_Accuracy_Test.R", echo=TRUE)
source("C:/Users/twink_cxd1o43/OneDrive/Desktop/University Notes + Work/Year 2/Dolphin Acoustics/R_Files_as696/Dolphin_Acoustics_VIP_Stats_2024-25_Sem2/MergeOfDataAndAnalysis/RF_Accuracy_Test.R", echo=TRUE)
source("C:/Users/twink_cxd1o43/OneDrive/Desktop/University Notes + Work/Year 2/Dolphin Acoustics/R_Files_as696/Dolphin_Acoustics_VIP_Stats_2024-25_Sem2/MergeOfDataAndAnalysis/RF_Accuracy_Test.R", echo=TRUE)
print("Run Number" + i)
source("C:/Users/twink_cxd1o43/OneDrive/Desktop/University Notes + Work/Year 2/Dolphin Acoustics/R_Files_as696/Dolphin_Acoustics_VIP_Stats_2024-25_Sem2/MergeOfDataAndAnalysis/RF_Accuracy_Test.R", echo=TRUE)
source("C:/Users/twink_cxd1o43/OneDrive/Desktop/University Notes + Work/Year 2/Dolphin Acoustics/R_Files_as696/Dolphin_Acoustics_VIP_Stats_2024-25_Sem2/MergeOfDataAndAnalysis/RF_Accuracy_Test.R", echo=TRUE)
print(paste("Run Number: ", i))
source("C:/Users/twink_cxd1o43/OneDrive/Desktop/University Notes + Work/Year 2/Dolphin Acoustics/R_Files_as696/Dolphin_Acoustics_VIP_Stats_2024-25_Sem2/MergeOfDataAndAnalysis/RF_Accuracy_Test.R", echo=TRUE)
source("C:/Users/twink_cxd1o43/OneDrive/Desktop/University Notes + Work/Year 2/Dolphin Acoustics/R_Files_as696/Dolphin_Acoustics_VIP_Stats_2024-25_Sem2/MergeOfDataAndAnalysis/RF_Accuracy_Test.R", echo=TRUE)
source("C:/Users/twink_cxd1o43/OneDrive/Desktop/University Notes + Work/Year 2/Dolphin Acoustics/R_Files_as696/Dolphin_Acoustics_VIP_Stats_2024-25_Sem2/MergeOfDataAndAnalysis/RF_Accuracy_Test.R", echo=TRUE)
source("C:/Users/twink_cxd1o43/OneDrive/Desktop/University Notes + Work/Year 2/Dolphin Acoustics/R_Files_as696/Dolphin_Acoustics_VIP_Stats_2024-25_Sem2/MergeOfDataAndAnalysis/RF_Accuracy_Test.R", echo=TRUE)
source("C:/Users/twink_cxd1o43/OneDrive/Desktop/University Notes + Work/Year 2/Dolphin Acoustics/R_Files_as696/Dolphin_Acoustics_VIP_Stats_2024-25_Sem2/MergeOfDataAndAnalysis/Random_Forest_Analysis.R", echo=TRUE)
source("C:/Users/twink_cxd1o43/OneDrive/Desktop/University Notes + Work/Year 2/Dolphin Acoustics/R_Files_as696/Dolphin_Acoustics_VIP_Stats_2024-25_Sem2/MergeOfDataAndAnalysis/Random_Forest_Analysis.R", echo=TRUE)
source("C:/Users/twink_cxd1o43/OneDrive/Desktop/University Notes + Work/Year 2/Dolphin Acoustics/R_Files_as696/Dolphin_Acoustics_VIP_Stats_2024-25_Sem2/MergeOfDataAndAnalysis/Random_Forest_Analysis.R", echo=TRUE)
source("C:/Users/twink_cxd1o43/OneDrive/Desktop/University Notes + Work/Year 2/Dolphin Acoustics/R_Files_as696/Dolphin_Acoustics_VIP_Stats_2024-25_Sem2/MergeOfDataAndAnalysis/RF_Accuracy_Test.R", echo=TRUE)
